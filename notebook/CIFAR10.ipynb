{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset\n",
    "\n",
    "The dataset used in this notebook is downloaded from [here](https://www.cs.toronto.edu/~kriz/cifar.html). CIFAR-10 dataset is a subset of 80 million tiny image dataset. CIFAR-10 consists of 60,000 images in total. Training data has around 50k images and the test data has 10k images. CIFAR-10 dataset has 10 categories with 6000 images in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages\n",
    "\n",
    "In this notebook, we will be mainly working with PyTorch for building and training ConvNets on CIFAR10 Dataset. Using PyTorch makes things more easier to understand however, whatever we do here must be similar in TensorFlow as well. \n",
    "\n",
    "*Note : The python scripts for this notebook will be slightly different from this notebook. But all the concepts used will be the same. Also please move this notebook to main project directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import ToTensor,Compose,RandomHorizontalFlip, Normalize, ToPILImage, RandomRotation, ColorJitter\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the downloaded dataset\n",
    "\n",
    "The dataset downloaded has 6 batches, `data_batch1, data_batch2,..., data_batch5` are the training batches. Each batch has 10k images in it. `test_batch` is the batch that is meant to be used for model testing. The test batch contains 10k images.\n",
    "\n",
    "The batches has been created using cPickle. Each batch is an array of shape (10000,3072) where 10,000 is number of images and 3072 is the pixel values of the image.\n",
    "\n",
    "Extracting the dataset according to the method suggested in the [CIFAR-10 Website](https://www.cs.toronto.edu/~kriz/cifar.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(filename):\n",
    "    with open(filename,\"rb\") as f:\n",
    "        batch_data = pickle.load(f,encoding=\"bytes\")\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] #Store all batches in a list\n",
    "for files in os.listdir(\"./cifar-10-batches-py/\"):\n",
    "    if \"_batch\" in files:\n",
    "        data.append(extract(os.path.join('./cifar-10-batches-py',files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset Class using `Dataset` Module\n",
    "\n",
    "Using the above method to extarct, we will now create a custom dataset class which inherits the `Dataset` class from `torch.utils.data` package. Creating this custom dataset class is essential as it will help us easily manage our dataset and apply the data augmentation during runtime. The `DataLoader` package takes full advantage of this custom dataset class. Instead of loading all the images at once, the `DataLoader` reads batches of data. Even though we already have batches of data in our dataset, creating this custom class allows us to use any batch_size. Currently, the batches_data have 10k images in them. Having so many images in one batch may not fit in memory. Hence to change this batch_size, we will create a custom dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10(Dataset):\n",
    "    \n",
    "    def __init__(self,root,train=True,transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.split = train\n",
    "        \n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.train_data = [file for file in os.listdir(root) if \"data_batch\" in file]\n",
    "        self.test_data = [file for file in os.listdir(root) if \"test_batch\" in file]\n",
    "                \n",
    "        data_split = self.train_data if self.split else self.test_data\n",
    "        \n",
    "        for files in data_split:\n",
    "            entry = self.extract(os.path.join(root,files))\n",
    "            self.data.append(entry[\"data\"])\n",
    "            self.targets.extend(entry[\"labels\"])\n",
    "                \n",
    "        self.data = np.vstack(self.data).reshape(-1, 3, 32, 32)\n",
    "        self.data = self.data.transpose((0, 2, 3, 1))\n",
    "        self.load_meta()\n",
    "        \n",
    "    def extract(self,filename):\n",
    "        with open(filename,\"rb\") as f:\n",
    "            batch_data = pickle.load(f,encoding=\"latin1\")\n",
    "        return batch_data  \n",
    "    \n",
    "    def load_meta(self):\n",
    "        path = os.path.join(self.root,\"batches.meta\")\n",
    "        with open(path,\"rb\") as infile:\n",
    "            data = pickle.load(infile,encoding=\"latin1\")\n",
    "            self.classes = data[\"label_names\"]\n",
    "            self.classes_to_idx = {_class:i for i,_class in enumerate(self.classes)}\n",
    "            \n",
    "    def plot(self,image,target=None):\n",
    "        if target is not None:\n",
    "            print(f\"Target :{target} class :{self.classes[target]}\")\n",
    "        plt.figure(figsize=(2,2))\n",
    "        plt.imshow(image.permute(1,2,0))\n",
    "        plt.show()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        image,target = self.data[idx],self.targets[idx]\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        return image,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CIFAR10(root=\"./cifar-10-batches-py\",train=True,\n",
    "                    transforms=Compose([\n",
    "                        ToTensor()]))\n",
    "test_set = CIFAR10(root=\"./cifar-10-batches-py\",train=False,\n",
    "                    transforms=Compose([\n",
    "                        ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_set[1036]\n",
    "img,label = batch\n",
    "train_set.plot(img,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ConvNet Model\n",
    "\n",
    "Now that we are done with constructing the dataset class, it's time to build a ConvNet model. We will also create a class which specifies the training configurations so that it becomes easier for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=8,stride=1,kernel_size=(3,3),padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=32,kernel_size=(3,3),padding=1,stride=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=(3,3),padding=1,stride=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=(3,3),padding=1,stride=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(3,3),stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=6*6*256,out_features=256)\n",
    "        self.fc2 = nn.Linear(in_features=256,out_features=128)\n",
    "        self.fc3 = nn.Linear(in_features=128,out_features=64)\n",
    "        self.fc4 = nn.Linear(in_features=64,out_features=10)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=(2,2),stride=2)\n",
    "        self.dropout = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "    def forward(self,x,targets):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1,6*6*256)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        logits = self.fc4(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    \n",
    "    def configure_optimizers(self,config):\n",
    "        optimizer = optim.Adam(self.parameters(),lr=config.lr,betas=config.betas,weight_decay=config.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Training Configuration Class\n",
    "\n",
    "I often see people just specify the training configurations directly. I don't prefer this way. We will create a simple training config class and pass that config class when we train our model. This makes it a neat way of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    \n",
    "    lr=3e-4\n",
    "    betas=(0.9,0.995)\n",
    "    weight_decay=5e-4\n",
    "    num_workers=0\n",
    "    max_epochs=10\n",
    "    batch_size=64\n",
    "    ckpt_path=None #Specify a model path here. Ex: \"./Model.pt\"\n",
    "    shuffle=True\n",
    "    pin_memory=True\n",
    "    verbose=True\n",
    "    \n",
    "    def __init__(self,**kwargs):\n",
    "        for key,value in kwargs.items():\n",
    "            setattr(self,key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Training Loop\n",
    "\n",
    "Now, we will be creating a simple training loop to train our model. It may look complicated but trust me, when you understand what is going on, it's simple. It also shows how other libraries like TensorFlow will hide some important stuff from you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,model,train_dataset,test_dataset,config):\n",
    "        self.model = model\n",
    "        self.train_dataset=train_dataset\n",
    "        self.test_dataset=test_dataset\n",
    "        self.config = config\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.test_losses = []\n",
    "        self.test_accuracies = []\n",
    "        \n",
    "        self.device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = self.model.to(self.device)\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        raw_model = self.model.module if hasattr(self.model,\"module\") else self.model\n",
    "        torch.save(raw_model.state_dict(),self.config.ckpt_path)\n",
    "        print(\"Model Saved!\")\n",
    "        \n",
    "    def train(self):\n",
    "        model,config = self.model,self.config\n",
    "        raw_model = self.model.module if hasattr(self.model,\"module\") else self.model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "        \n",
    "        def run_epoch(split):\n",
    "            is_train = split==\"train\"\n",
    "            if is_train:\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval() #important don't miss this. Since we have used dropout, this is required.\n",
    "            data = self.train_dataset if is_train else self.test_dataset\n",
    "            loader = DataLoader(data,batch_size=config.batch_size,\n",
    "                                shuffle=config.shuffle,\n",
    "                                pin_memory=config.pin_memory,\n",
    "                                num_workers=config.num_workers)\n",
    "            \n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "            \n",
    "            pbar = tqdm(enumerate(loader),total=len(loader)) if is_train and config.verbose else enumerate(loader)\n",
    "            for it,(images,targets) in pbar:\n",
    "                images = images.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                num_samples += targets.size(0)\n",
    "                \n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    #forward the model\n",
    "                    logits,loss = model(images,targets)\n",
    "                    loss = loss.mean()\n",
    "                    losses.append(loss.item())\n",
    "                    \n",
    "                with torch.no_grad():\n",
    "                    predictions = torch.argmax(logits,dim=1) #softmax gives prob distribution. Find the index of max prob\n",
    "                    correct+= predictions.eq(targets).sum().item()\n",
    "                    accuracies.append(correct/num_samples)\n",
    "                    \n",
    "                if is_train:\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    if config.verbose:\n",
    "                        pbar.set_description(f\"Epoch:{epoch+1} iteration:{it+1} | loss:{np.mean(losses)} accuracy:{np.mean(accuracies)} lr:{config.lr}\")\n",
    "                    \n",
    "                    self.train_losses.append(np.mean(losses))\n",
    "                    self.train_accuracies.append(np.mean(accuracies))\n",
    "            \n",
    "            if not is_train:\n",
    "                test_loss = np.mean(losses)\n",
    "                if config.verbose:\n",
    "                    print(f\"\\nEpoch:{epoch+1} | Test Loss:{test_loss} Test Accuracy:{correct/num_samples}\\n\")\n",
    "                self.test_losses.append(test_loss)\n",
    "                self.test_accuracies.append(correct/num_samples)\n",
    "                return test_loss\n",
    "                \n",
    "        best_loss = float('inf')\n",
    "        test_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(config.max_epochs):\n",
    "            run_epoch('train')\n",
    "            if self.test_dataset is not None:\n",
    "                test_loss = run_epoch(\"test\")\n",
    "                \n",
    "            good_model = self.test_dataset is not None and test_loss < best_loss\n",
    "            if config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumb Baselines\n",
    "\n",
    "In this section we will get a dumb baseline score which we can use to compare our model against. To get dumb baseline scores, we will pass a zero input image and ask our model to predict something. By doing this, we can see whether our model has learnt to extract any information from the images at all when we pass an actual image from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_images = torch.zeros([10,3,32,32])\n",
    "labels = torch.tensor(data[0][b\"labels\"][:10])\n",
    "\n",
    "net = ConvNet()\n",
    "optimizer = optim.Adam(net.parameters(),lr=3e-4)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    logits,loss = net(zero_images,labels)\n",
    "    losses.append(loss.item())\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(\"Loss :\",np.mean(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we cannot achieve a loss lower than 2.03. We better beat this when we provide our model with actual data from the dataset. If we cannot beat this then, it suggests that our model is not learning to extarct any information from the images we show it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit Test\n",
    "\n",
    "Our training loop is ready! We now have to check if our model is wired properly and that it can overfit a single batch of training data. Doing this will save us a lot of time. Overfitting a small batch of data will tell us that the model is capable of learning and that there is no bug in our model. If the overfit test is not done, and we start training our model with the full dataset directly, we will not be able to find a bug and we will waste time in training a network that will not learn anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CIFAR10(root=\"./cifar-10-batches-py\",train=True,transforms=ToTensor())\n",
    "\n",
    "small_batch,train_data = random_split(train_set,[10,len(train_set)-10]) #take 10 examples from the trainset\n",
    "\n",
    "trainconfig = TrainingConfig(max_epochs=200,batch_size=10,weight_decay=0,num_workers=0)\n",
    "trainer = Trainer(model,train_dataset=small_batch,test_dataset=None,config=trainconfig)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are able to overfit successfully. This indicates that there are no bugs in our model architecture. This step is very important as it will save a lot of time in future. Can't overfit? Then we need to take a look at our model architecture to resolve some bugs or create a new one altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Batch Images\n",
    "\n",
    "Another important thing to observe is the dataset itself. Visualizing what goes into your model is very essential. It is at this stage that you will find certain pre-processing errors that may have happened but you didn't know that it had occured. Uunfortunately, our model doesn't know which data is bad and which data is good. It takes in everything. However, model may figure out certain pre-processing errors and will just ignore that example. But this will not happen all the time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=1024,shuffle=True)\r\n",
    "batch = iter(train_loader)\r\n",
    "images,labels = next(batch)\r\n",
    "grid = make_grid(images,nrow=64)\r\n",
    "plt.figure(figsize=(50,50))\r\n",
    "plt.imshow(grid.permute(1,2,0))\r\n",
    "# plt.show()\r\n",
    "plt.savefig(\"./log/images/CIFAR10.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set,batch_size=64,shuffle=True)\n",
    "batch = iter(train_loader)\n",
    "images,labels = next(batch)\n",
    "grid = make_grid(images,nrow=8)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above image is a subset of training images of `batch_size=64`. Since we are not using any exotic data augmentations like RandomHorizontalFlips, ColorJittering and stuff as of now, we need not worry too much about having pre-processing errors in our dataset. However, pre-processing errors are very common in real world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "There are many methods of doing hyperparameter optimization. You may be familiar with GridSearchCV that is offen used in machine learning. Here we will not be using GridSearchCV to find the right values for our hyper parameters. Instead we will use the coarse to fine strategy to find descent values for them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = ConvNet()\n",
    "train_set = CIFAR10(root=\"./cifar-10-batches-py\",train=True,\n",
    "                    transforms=Compose([\n",
    "                        ToTensor(),\n",
    "                        Normalize(mean=(0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  std=(0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                    ]))\n",
    "\n",
    "test_set = CIFAR10(root=\"./cifar-10-batches-py\",train=False,\n",
    "                   transforms=Compose([\n",
    "                        ToTensor(),\n",
    "                        Normalize(mean=(0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  std=(0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a very low learning rate of 1e-6 first and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainingConfig(max_epochs=7,lr=1e-6,batch_size=64,weight_decay=0,num_workers=0,verbose=True)\n",
    "trainer = Trainer(model=Model,train_dataset=train_set,test_dataset=test_set,config=train_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a learning rate of 1e-6, the train_loss is barely moving. Suggests that learning rate is too low.\n",
    "\n",
    "We will try a learning rate of 1e-3 and check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = ConvNet() #reinit the model parameters\n",
    "train_config = TrainingConfig(max_epochs=10,lr=1e-3,batch_size=64,weight_decay=0,num_workers=0,verbose=True)\n",
    "trainer = Trainer(model=Model,train_dataset=train_set,test_dataset=test_set,config=train_config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the loss of both train_set and the test_set is going down. Which is a good sign. Hence we will search for values closer to 1e-3 in log space so that we can get better values for our hyperparameters.\n",
    "\n",
    "*Note : In the above training process we searched for a good learning rate first. Learning rate affects your model the most. Other parameters come next.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runs in range(20):\n",
    "    lr = 10**(np.random.uniform(-3,-5))\n",
    "    weight_decay = 10**(np.random.uniform(-4,-5))\n",
    "    \n",
    "    Model = ConvNet()\n",
    "    training_config = TrainingConfig(max_epochs=5,lr=lr,weight_decay=weight_decay,batch_size=64,verbose=False)\n",
    "    trainer = Trainer(model=Model,train_dataset=train_set,test_dataset=test_set,config=training_config)\n",
    "    trainer.train()\n",
    "    val_acc = np.mean(trainer.test_accuracies)\n",
    "    print(f\"val_acc:{val_acc} lr:{lr} reg:{weight_decay} ({runs+1}/{len(range(20))})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above cell will take some time to get executed. \n",
    "\n",
    "Inference from the above hyperparameter optimization process:\n",
    "\n",
    "We can notice that we are getting good results when the learning rate is between 1e-3 and 1e-4. We will now refine our search space to get much better values.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runs in range(20):\n",
    "    lr = 10**(np.random.uniform(-3,-4))\n",
    "    weight_decay = 10**(np.random.uniform(-4,-5))\n",
    "    \n",
    "    Model = ConvNet()\n",
    "    training_config = TrainingConfig(max_epochs=5,lr=lr,weight_decay=weight_decay,batch_size=64,verbose=False)\n",
    "    trainer = Trainer(model=Model,train_dataset=train_set,test_dataset=test_set,config=training_config)\n",
    "    trainer.train()\n",
    "    val_acc = np.mean(trainer.test_accuracies)\n",
    "    print(f\"val_acc:{val_acc} lr:{lr} reg:{weight_decay} ({runs+1}/{len(range(20))})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above tuning process, we can see that learning rates lower than 0.6e-4 tend to work better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for runs in range(20):\n",
    "#     lr = 10**(np.random.uniform(-4,-3))\n",
    "    lr = 0.0009446932175584296\n",
    "    weight_decay = 10**(np.random.uniform(-3,-4))\n",
    "    \n",
    "    Model = ConvNet()\n",
    "    training_config = TrainingConfig(max_epochs=5,lr=lr,weight_decay=weight_decay,batch_size=64,verbose=False)\n",
    "    trainer = Trainer(model=Model,train_dataset=train_set,test_dataset=test_set,config=training_config)\n",
    "    trainer.train()\n",
    "    val_acc = np.mean(trainer.test_accuracies)\n",
    "    print(f\"val_acc:{val_acc} lr:{lr} reg:{weight_decay} ({runs+1}/{len(range(20))})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note : I could not find any other good hyperparameter values other than for the one we got ~60.6% accuracy `(val_acc:0.6063799999999999 lr:0.0009446932175584296 reg:0.00011257445443209662 (9/20))`. So we will we using those values itself for training the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ConvNet on CIFAR-10\n",
    "\n",
    "We can now train our model on the dataset we have downloaded. Hopefully things go well :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = ConvNet()\n",
    "# model.load_state_dict(torch.load(\"./Model.pt\")) #Uncomment this to load pre-trained weights\n",
    "train_set = CIFAR10(root=\"./cifar-10-batches-py\",train=True,\n",
    "                    transforms=Compose([\n",
    "                        ToTensor(),\n",
    "                        RandomHorizontalFlip(),\n",
    "                        RandomRotation(degrees=10),\n",
    "                        ColorJitter(brightness=0.5),\n",
    "                        Normalize(mean=(0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  std=(0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                    ]))\n",
    "\n",
    "test_set = CIFAR10(root=\"./cifar-10-batches-py\",train=False,\n",
    "                   transforms=Compose([\n",
    "                        ToTensor(),\n",
    "                        Normalize(mean=(0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  std=(0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                    ]))\n",
    "\n",
    "train_config = TrainingConfig(max_epochs=300,\n",
    "                              lr=0.0009446932175584296,\n",
    "                              weight_decay=0.00011257445443209662,\n",
    "                              ckpt_path=\"./models/Final_Model.pt\",\n",
    "                              batch_size=64,\n",
    "                              num_workers=0)\n",
    "\n",
    "trainer = Trainer(model,train_dataset=train_set,\n",
    "                  test_dataset=test_set,config=train_config)\n",
    "\n",
    "### Uncomment the following if you have already trained a model and want to continue training ###\n",
    "# trainer.train_losses = torch.load(\"./train_losses.pt\")\n",
    "# trainer.train_accuracies = torch.load(\"./train_accuracies.pt\")\n",
    "# trainer.test_losses = torch.load(\"./test_losses.pt\")\n",
    "# trainer.test_accuracies = torch.load(\"./test_accuracies.pt\")\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Model.state_dict(),\"./models/Model300.pt\") #Uncomment this if you want to save the model \n",
    "torch.save(trainer.train_losses,\"./log/train_losses.pt\")\n",
    "torch.save(trainer.train_accuracies,\"./log/train_accuracies.pt\")\n",
    "torch.save(trainer.test_losses,\"./log/test_losses.pt\")\n",
    "torch.save(trainer.test_accuracies,\"./log/test_accuracies.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have trained the model for around 400 epochs and the final model achieves a test accuracy of 87.27% training for longer could increase the accuracy. CIFAR-10 dataset is a hard dataset. Human level performance is about 94% and for our model to achieve that it will take a long time.\n",
    "\n",
    "The best model achieved has a test loss of 0.380 and a test accuracy of 87.35%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-1 and Top-5 Accuracies\n",
    "\n",
    "Now we will measure the TOP-1 and TOP-5 Accuracies of our model for both train and test datasets. TOP-1 Accuracy is calculated by calculating the number of accurate predictions out of total predictions made by our model. Top-5 accuracy is the accuracy obtained when the the correct label for a class lies within the first 5 perdictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Model = ConvNet()\n",
    "Best_Model = ConvNet()\n",
    "Final_Model.load_state_dict(torch.load(\"./models/Final_Model.pt\"))\n",
    "Best_Model.load_state_dict(torch.load(\"./models/Best_Model.pt\"))\n",
    "\n",
    "train_set = CIFAR10(root=\"./cifar-10-batches-py\",train=True,\n",
    "                    transforms=Compose([\n",
    "                        ToTensor(),\n",
    "                        RandomHorizontalFlip(),\n",
    "                        RandomRotation(degrees=10),\n",
    "                        ColorJitter(brightness=0.5),\n",
    "                        Normalize(mean=(0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  std=(0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                    ]))\n",
    "\n",
    "test_set = CIFAR10(root=\"./cifar-10-batches-py\",train=False,\n",
    "                   transforms=Compose([\n",
    "                        ToTensor(),\n",
    "                        Normalize(mean=(0.4913997551666284, 0.48215855929893703, 0.4465309133731618),\n",
    "                                  std=(0.24703225141799082, 0.24348516474564, 0.26158783926049628))\n",
    "                    ]))\n",
    "\n",
    "train_loader = DataLoader(train_set,batch_size=64,shuffle=True,num_workers=0)\n",
    "test_loader = DataLoader(test_set,batch_size=64,shuffle=True,num_workers=0)\n",
    "\n",
    "for model in [\"Final_Model\",\"Best_Model\"]:\n",
    "    for loader in [train_loader,test_loader]:\n",
    "        top1_accuracy = []\n",
    "        top5_accuracy = []\n",
    "        Model = Final_Model if model==\"Final_Model\" else Best_Model\n",
    "        for it, (images,targets) in enumerate(loader):\n",
    "            logits,loss = Model(images,targets)\n",
    "            acc1, acc5 = accuracy(logits, targets, topk=(1, 5))\n",
    "            top1_accuracy.append(acc1)\n",
    "            top5_accuracy.append(acc5)\n",
    "        \n",
    "        split = \"train\" if loader==train_loader else \"test\"\n",
    "        print(f\"Model : {model}\\nsplit : {split}\")\n",
    "        print(\"Top1 Accuracy :\",np.mean(top1_accuracy))\n",
    "        print(\"Top5 Accuracy :\",np.mean(top5_accuracy))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Graphs\n",
    "\n",
    "You can use the training metrics stored in trainer class to plot some loss and accuracy curves. I did not get enough time to code this. I think you can do it on your own. \n",
    "\n",
    "```python\n",
    "trainer.train_losses = torch.load(\"./train_losses.pt\")\n",
    "trainer.train_accuracies = torch.load(\"./train_accuracies.pt\")\n",
    "trainer.test_losses = torch.load(\"./test_losses.pt\")\n",
    "trainer.test_accuracies = torch.load(\"./test_accuracies.pt\")\n",
    "```\n",
    "\n",
    "The above lists in trainer classes will have training losses, accuracies, test losses and test accuracies.\n",
    "\n",
    "You can also plot Confusion Matrix using Sklean library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
